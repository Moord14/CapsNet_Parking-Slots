{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPzY-F31h20k"
   },
   "source": [
    "## E-tivity   - Binary Logistic regression - Task A\n",
    "There are many situations where some $y$ should be a noisy\n",
    "representation of some function of $x_1,\\ldots,x_p$, but the \n",
    "response outcomes $y$ are binary.\n",
    "\n",
    "Binary logistic regression corresponds to this probabilistic model\n",
    "\n",
    "$$\n",
    "p\\left(y|w^T x \\right)=\\phi(w^T x)^{y}(1-\\phi(w^T x ))^{1-y}  \\text{ for } y=0,1\n",
    "$$\n",
    "\n",
    "where $w \\in \\mathbb{R}^p$ is the vector if coefficient and $\\phi(\\cdot)$ is some sigmoid function,e.g.,\n",
    "\n",
    "* Logistic function: $\\phi(x)=1/(1+e^{-x})$\n",
    "* probit function: $\\phi(x)=0.5(1+\\text{erf}(x/\\sqrt{2}))$\n",
    "\n",
    "\n",
    "Given $N$ training data, the likelihood function assuming that all the observations in the sample are independently Bernoulli distributed is\n",
    "\n",
    "$$\n",
    "{\\displaystyle {\\begin{aligned}L(\\theta \\mid x)&=\\Pr(Y\\mid X;\\theta )\\\\&=\\prod_{i=1}^Np(y_{i}\\mid x_{i};\\theta )\\\\&=\\prod _{i=1}^N\\phi(w^T x_{i})^{y_{i}}(1-\\phi(w^T x_{i}))^{(1-y_{i})}\\end{aligned}}}\n",
    "$$\n",
    "\n",
    "Our goal is to find the maximum likelihood estimate of the vector of the parameters $w$, that is to solve the optimisation problem\n",
    "\n",
    "$$\n",
    "\\hat{w} =\\arg\\max_{w \\in \\mathbb{R}^p} \\prod _{i=1}^N\\phi(w^T x_{i})^{y_{i}}(1-\\phi(w^T x_{i}))^{(1-y_{i})}\n",
    "$$\n",
    "\n",
    "The product of probability (that are numbers between $0$ and $1$) can become very small for large $N$\n",
    "and can lead to arithmetic underflow problems.\n",
    "To avoid this problem, one typically maximises the logarithm of the likelihood function (log transform). In fact, one can prove that for any nonnegative function\n",
    "\n",
    "$$\n",
    "\\arg\\max_{x} f(x) = \\arg\\max_{x} \\log(x)  \n",
    "$$\n",
    "\n",
    "and, therefore, we actually solve:\n",
    "\n",
    "$$\n",
    "\\hat{w} =\\arg\\max_{w \\in \\mathbb{R}^p}  \\sum _{i=1}^{N}y_{i}\\log(\\phi(w^T x_{i}))+(1-y_{i})\\log(1-\\phi(w^T x_{i}))\n",
    " $$\n",
    "\n",
    "which is maximized using optimization techniques.\n",
    "\n",
    "**Reguralisation**\n",
    "In general-recipe ML, reguralisation is used to reduce the problem of overfitting. \n",
    "Therefore, actually the problem we aim to solve is\n",
    "\n",
    "$$\n",
    "\\hat{w} =\\arg\\max_{w \\in \\mathbb{R}^p}  \\sum _{i=1}^{N}y_{i}\\log(\\phi(w^T x_{i}))+(1-y_{i})\\log(1-\\phi(w^T x_{i})) + \\alpha||w||_2^2\n",
    " $$\n",
    " \n",
    " where $ \\alpha$ is the reguralisation constant (a design parameter that is usually selected via cross-validation, note that $ \\alpha\\geq0$) and $||w||_2^2=\\sum_{=1}^{p} w_i^2$. \n",
    "\n",
    "\n",
    "We assume that you know how logistic regression works: (i) how to compute the predicted probability; (ii) how to return the predicted class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq3-1ftGh20s"
   },
   "source": [
    "## Task A\n",
    "Part 1 Theorethical questions: \n",
    "\n",
    "    What type of optimisation problem is it?\n",
    "    1. Is it univariate or multivariate?\n",
    "    2. Is it linear or nonlinear?\n",
    "    3. Is it convex or not? (google it and cite your font, how is the fast way to prove it?)\n",
    "    4. Is it constrained?\n",
    "    \n",
    "Part 2: The goal of this part is to reimplement **totally** from scratch the class LogisticRegression, that mimics the corresponding class in `Sklearn`. \n",
    "* The class must be flexible: the user can choose any type of differentiable (w.r.t. the parameters $w$) sigmoid function and the class must work for any number of features $p$ (suggestion: use `autograd` library). \n",
    "* As optimizer you should use `local_descent` with  `line_research`.\n",
    "\n",
    "\n",
    "\n",
    "**Submission (README):** Upload your notebook to the Gitlab repository and submit a post to your forum.\n",
    "* Please follow this format for your filename: **CS5014_E-tivity1A -YourName_YourStudentID.ipynb** and add your full name and student number at the top of your submitted notebook.\n",
    "* Your forum post should include a few words about your submission and a link to your notebook in Gitlab.\n",
    "* The notebook must only include the source code of the implemented class and optimization algorithm\n",
    "* A short example about how to use it (e.g., how to run your code on the iris dataset, see the example below) so your peers can understand how to run your code. \n",
    "* You must **not** include any tests you performed to verify that your code is properly working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CykklwO0h20t"
   },
   "outputs": [],
   "source": [
    "#Skeleton code\n",
    "class Generic_LogisticRegression():\n",
    "    def __init__(self, θ, sigmoid, α=1.0):\n",
    "        self.θ = θ #initial value of the parameter\n",
    "        self.sigmoid = sigmoid #a sigmoid function that squezes the input into [0,1]\n",
    "        self.α = α #reguralisation parameter\n",
    "        \n",
    "    def fit(self,X,y,optimization_method,Optimparam): \n",
    "        \n",
    "        #first define the cost function as given above\n",
    "        def cost(θ):\n",
    "            z = np.dot(X, θ)\n",
    "            sig=self.sigmoid(z)\n",
    "            p1 = y * np.log(sig)\n",
    "            p0 = (1 - y) * np.log(1 - sig)\n",
    "            return (-np.sum(p1 + p0))  + self.α * sum(np.square(θ[:]))\n",
    "        \n",
    "        \n",
    "        #automatic diff gradient function\n",
    "\n",
    "        gradcost=grad(cost)\n",
    "\n",
    "        #call the optimizer\n",
    "        θ = optimization_method(self.θ, cost, gradcost, steps=Optimparam['steps'], α=self.α, ϵ_x =  Optimparam['ϵ_x'], ϵ_d =  Optimparam['ϵ_d'], plotting=False)\n",
    "        self.θ = θ[-1]\n",
    "        ;\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        #predict probability of the class\n",
    "\n",
    "        return self.sigmoid(np.dot(X, self.θ))\n",
    "        ;\n",
    "        \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        #predict class\n",
    "\n",
    "        return [1 if x >= threshold else 0 for x in self.predict_proba(X)]\n",
    "        ;\n",
    "\n",
    "#your implementation of the optimization algorithm\n",
    "\n",
    "def minimize(β,cost, gradient,steps, α, ϵ_x=0.0001, ϵ_d=0.0001, plotting=False):\n",
    "    \n",
    "    # the optimisation algorithm we will use is local descent with line search \n",
    "    def line_search(f, x, d, ϵ_x, ϵ_d):\n",
    "        term = False\n",
    "        if np.linalg.norm(d) < ϵ_d:\n",
    "            print(\"Gradient norm below threshold\")\n",
    "            term = True\n",
    "            return x, term \n",
    "        objective = lambda α : f(x + α*d)\n",
    "        a, b = bracket_minimum(objective) #function defined below\n",
    "        a, b  = golden_section_search(objective, a, b, max_iter = 5, plotting=False) #function defined below\n",
    "        α = (a+b)/2\n",
    "        if np.linalg.norm(x+a*d-x)<ϵ_x:\n",
    "            print(\"Step Tolerance below threshold\")\n",
    "            term = True\n",
    "        return x + α*d, term\n",
    "\n",
    "    def local_descent(β,cost, gradient,steps, α, ϵ_x, ϵ_d,  plotting=False):\n",
    "        Tmp = [β]\n",
    "        for iteration in range(steps):\n",
    "            d = gradient(β)\n",
    "            d = -d/np.linalg.norm(d) \n",
    "            #print(β,d)\n",
    "            β, term = line_search(cost, β, d, ϵ_x=ϵ_x, ϵ_d=ϵ_d)\n",
    "            if term == True:\n",
    "                break\n",
    "            Tmp.append(β)\n",
    "        #print(np.array(Tmp))\n",
    "        return Tmp\n",
    "\n",
    "\n",
    "    #bracket_minimum required for line search function\n",
    "    def bracket_minimum(f, β=0, s=1e-2, k=2.0): #From the Book, pag.36\n",
    "        a, ya = β, f(β)\n",
    "        b, yb = a + s, f(a + s)\n",
    "        if yb > ya:\n",
    "            a, b = b, a\n",
    "            ya, yb = yb, ya\n",
    "            s = -s\n",
    "        while True:\n",
    "            c, yc = b + s, f(b + s)\n",
    "            if yc > yb:\n",
    "                return (a, c) if a<c else (c, a)        \n",
    "            a, ya, b, yb = b, yb, c, yc\n",
    "            s *= k\n",
    "\n",
    "    from math import sqrt\n",
    "    ϕ = (1 + sqrt(5))/2\n",
    "\n",
    "    #golden section search also required for line search\n",
    "    def golden_section_search(f, a, b, max_iter, plotting=True): #from the Book pag.41\n",
    "        a0 =a\n",
    "        b0=b\n",
    "        ρ =  ϕ-1\n",
    "        d = ρ * b + (1 - ρ)*a\n",
    "        yd = f(d)\n",
    "        for i in range(max_iter-1):\n",
    "            c = ρ*a + (1 - ρ)*b\n",
    "            yc = f(c)\n",
    "            if yc < yd:\n",
    "                b, d, yd = d, c, yc\n",
    "            else:\n",
    "                a, b = b, c   \n",
    "            if plotting==True:\n",
    "                plt.figure()\n",
    "                xx = np.linspace(a0,b0,100)\n",
    "                plt.plot(xx,f(xx))\n",
    "                plt.scatter(np.array([a,b]),np.array([a,b])*0)\n",
    "                plt.scatter(np.array([a,b]),np.array([f(a),f(b)]))\n",
    "\n",
    "        return (a, b) if a<b else (b, a)\n",
    "\n",
    "    return local_descent(β,cost, gradient,steps,α , ϵ_x, ϵ_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhlxDzXrh20u"
   },
   "source": [
    "#### Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhibVn5Nh20v",
    "outputId": "a6fe392f-89fe-4ffe-8c1f-a7fcc17835c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4151075])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autograd.scipy.special import erf\n",
    "import autograd.numpy as np  # \n",
    "from autograd import grad    #\n",
    "\n",
    "graderf = grad(erf)\n",
    "graderf(np.array([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2ijAYtXh20w",
    "outputId": "0b6b54a7-a170-45f6-d443-9e69a71f432e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47,  3],\n",
       "       [ 1, 49]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "ind = np.where((y==1) | (y==2))[0] # we only have two classes now\n",
    "X = X[ind,:]\n",
    "y = y[ind]\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "clf.predict(X[:2, :])\n",
    "\n",
    "clf.predict_proba(X[:4, :])\n",
    "\n",
    "clf.predict(X)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y, clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-22\\lib\\site-packages\\autograd\\tracer.py:48: RuntimeWarning: divide by zero encountered in log\n",
      "  return f_raw(*args, **kwargs)\n",
      "<ipython-input-1-d8979604d63c>:15: RuntimeWarning: invalid value encountered in multiply\n",
      "  p0 = (1 - y) * np.log(1 - sig)\n",
      "<ipython-input-1-d8979604d63c>:16: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (-np.sum(p1 + p0))  + self.α * sum(np.square(θ[:]))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-22\\lib\\site-packages\\autograd\\tracer.py:48: RuntimeWarning: overflow encountered in square\n",
      "  return f_raw(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Repeat using our Generic_LogisticRegression() rather than sklearns\n",
    "\n",
    "#Define the sigmoid function we'll use\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "# Get initial values for the weights \n",
    "θ = np.random.rand(1, X.shape[1]).reshape(-1)\n",
    "\n",
    "# define parameters for Optimparam\n",
    "Optimparam = {\n",
    "    'steps':5000, \n",
    "    'ϵ_x':0.00001, \n",
    "    'ϵ_d':0.00001}\n",
    "\n",
    "# Fit the model \n",
    "GLR = Generic_LogisticRegression(θ, sigmoid, α=0.001)\n",
    "GLR.fit(X, y, minimize, Optimparam)\n",
    "\n",
    "# predict probabilities\n",
    "ypred_proba = GLR.predict_proba(X)\n",
    "\n",
    "# predict classes\n",
    "ypred = GLR.predict(X)\n",
    "np.array([ypred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "E-tivity1_TaskA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
